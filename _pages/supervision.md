---
layout: archive
title: "Supervision"
permalink: /supervision/
author_profile: true
---

I will be supervising Part II, Part III, and M.Phil. projects for students currently at Cambridge.
If you are interested in working with me, please feel free to email me with your <span style="color: rgb(203, 157, 255);">CV</span> and <span style="color: rgb(203, 157, 255);">transcript</span> (email: XYZ, where X=yc632, Y=@ and Z=cam.ac.uk). Before sending the email, please take a look at my <a href="https://scholar.google.co.uk/citations?user=8P23zSkAAAAJ"   style="color: rgb(203, 157, 255);">Google Scholar</a> to better understand my research area.

Research Interests
-----------------
Broadly speaking, I am interested in Natural Language Processing (NLP). Specific research of my interests includes: Automatic Fact-Checking, Text Summarisation (particularly controllable text summarisation and dialogue summarisation), and Large Language Models (in particular, evaluating hallucination in LLMs and improving their factuality). 

Note that I will NOT supervise projects outside the scope of NLP/AI, but I am open to interdisciplinary projects such as AI for Science (AI4Science) and NLP for Education (NLP4Education).

Potential Topics
-----------------
There are some potential topics for Part II projects.

* **Watermarking for LLMs**: Since the proposal of Large language models (LLMs), humans have abused them for generating texts in such as coursework and academic papers, which can further lead to problems such as copyright and misconduct. Watermarking is a technology that can insert unique, imperceptible patterns or codes into text or images to indicate the source or verify the authenticity of the content. In the context of LLMs, watermarking can serve as a tool to improve the detection of LLM-generated text, helping to address issues like academic dishonesty, copyright infringement, and the unethical use of AI-generated content.
  
  In this project, students will be asked to implement different watermark methods and detection methods as described in <a href="https://arxiv.org/pdf/2301.10226" style="color: rgb(203, 157, 255);">this paper</a>. They will apply the methods to generative models such as BART, GPT2 or T5. Also, they will conduct a human evaluation to judge whether watermarked text can be detected by humans. As an extension, they can explore how robust the watermark is, e.g., corrupt the watermarked text and see whether it can still be detected by the model.

<!-- * <span style="color: rgb(203, 157, 255);"><strong>Entity and Relation Extraction based on Hearst Pattern</strong></span> : <a href="https://frcchang.github.io/" style="color: rgb(203, 157, 255);">Yue Zhang</a> -->
<!-- * **Information Extraction based on Hearst Pattern**: Extracting entities, events and their relations from texts is useful. However, training neural models requires large annotation, which is costly. <a href="https://aclanthology.org/C92-2082/" style="color: rgb(203, 157, 255);">Hearst Pattern</a> is an unsupervised method that can iteratively mine entities or events under a specific relation, and patterns that can be used to identify entities or events under the relation. In this project, students will be asked to implement a Hearst based method to extract entities and their relation from text in a speficic domain (e.g., medicine or academic). Students are required to have basic understanding of NLP knowledge, and have a strong Python programming skill. Also, they should be familiar with NLP tools, such as syntactic parsers. -->

<!-- * **Abstractive Text Summarisation**: Text summarisation is to generate a concise piece of text that can express the main content of a long document or dialogue. In this project, students will be asked to implement an algorithm or a neural network for text summarisation (probably <a href="https://aclanthology.org/2021.findings-acl.449/" style="color: rgb(203, 157, 255);">dialogue summarisation</a>), and analyse their results. Students are required to have programming abilities in Python and Pytorch, and have the experience of training neural models. Note that this project requires skills in deep learning and NLP, which is not normally in the courses before the 4-th year. If you want to do this project as your Part II project, please make sure that you have relevant experience. -->

<!-- * **Question Answering using LLMs**: Large Language Models (LLMs) have shown strong abilities in NLP tasks in a zero-shot setting. By directly prompting LLMs with the input, LLMs can generate the answers to it. Previous work has shown that more advanced technologies such as Chain-of-Thoughts prompting and <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html" style="color: rgb(203, 157, 255);">Tree-of-Thoughts prompting</a> can further boost the performance. In this project, students will be asked to implement tree-of-thoughts for LLMs (LlaMA-based models) on Question Answering (QA) tasks. Students are required to have a strong understanding of NLP, and a strong Python programing skill. Also, students should have the experience of using large language models, in particular LlaMA (note not OpenAI API). If you want to do this project as your Part II project, please make sure that you have strong relevant experience. -->
  
For Part III and M.Phil. projects, they are more open research questions. You can either advance the above project or propose your own idea. Please drop me an email for further discussion.


Students/Mentees
-----------------
I am fortunate to work/have worked with a number of amazing students, with many of whom have progressed to D.Phil. programmes at world-leading institutes.

<!-- ### Current Students -->

### Past Students

* <a href="https://www.linkedin.com/in/shiduo-qian-a59261131/?original_referer=https%3A%2F%2Fwww%2Egoogle%2Ecom%2F&originalSubdomain=ca" style="color: rgb(203, 157, 255);">Shiduo Qian</a> (Co-supervised with Prof. <a href="https://frcchang.github.io/" style="color: rgb(203, 157, 255);">Yue Zhang</a> at Westlake University [Jul 2020 - Jan 2021], working on dialogue summarisation and financial data analysis. Undergraduate and postgraduate at Imperial College London [2017 - 2021]. Now Modeling/Forecasting Senior Analyst at TD Bank. Incoming M.Sc. student at Georgia Institute of Technology)

* <a href="https://chenllliang.github.io/about/"   style="color: rgb(203, 157, 255);">Liang Chen</a> (Co-supervised with Prof. <a href="https://frcchang.github.io/" style="color: rgb(203, 157, 255);">Yue Zhang</a> at Westlake University [Jun 2020 - Oct 2021], working on dialogue summarisation. Undergraduate at Jinlin University [2018 - 2022]. Now D.Phil. student at Peking University)
    
* <a href="https://cyber-e-j.github.io/"   style="color: rgb(203, 157, 255);">Yijie Zhou</a> (Co-supervised with Prof. <a href="https://frcchang.github.io/" style="color: rgb(203, 157, 255);">Yue Zhang</a> at Westlake University [Sep 2022 - Jan 2023], working on cross-lingual summarisation. Undergraduate at Zhejiang University [2020 - 2024]. Incoming D.Phil. student at University of Cambridge)


<!-- * <a href="https://ying-hui-he.github.io/"   style="color: rgb(203, 157, 255);">Yinghui Gracie He</a> (Co-supervised with <a href="https://dnaihao.github.io/" style="color: rgb(203, 157, 255);">Naihao Deng</a> at University of Michigan [2023], working on theory of mind. Undergraduate at University of Michigan [2020 - 2024]. Incoming D.Phil. student at Princeton University)

* <a href="https://www.linkedin.com/in/yufan-wu-a27b6b24b/"   style="color: rgb(203, 157, 255);">Yufan Wu</a> (Co-supervised with <a href="https://dnaihao.github.io/" style="color: rgb(203, 157, 255);">Naihao Deng</a> at University of Michigan [2023], working on theory of mind. Undergraduate at University of Michigan [2020 - 2024]. Incoming D.Phil. student at Ohio State University) -->


* Yinghao Yang (Co-supervised with Prof. <a href="https://frcchang.github.io/" style="color: rgb(203, 157, 255);">Yue Zhang</a> at Westlake University [Jul 2023 - Dec 2023], working on evaluating LLMs. Undergraduate at Westlake University [2023 - present])

* Pingchuan (Maestro) Yan (Co-supervised with Prof. <a href="https://frcchang.github.io/" style="color: rgb(203, 157, 255);">Yue Zhang</a> at Westlake University [Jul 2023 - Jul Dec 2023], working on meta-evaluation of LLMs for MT. Undergraduate at University College London [2023 - present])
